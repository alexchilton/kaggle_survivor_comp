{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T19:46:17.425224Z",
     "start_time": "2024-12-19T19:46:17.422073Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from colorama import Fore, Back, Style\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T20:40:15.599255Z",
     "start_time": "2024-12-19T20:40:15.303787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = 'equity-post-HCT-survival-predictions-1'\n",
    "\n",
    "\n",
    "#train_data = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\n",
    "#test_data  = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')\n",
    "\n",
    "train = pd.read_csv(DATA_DIR +'/train.csv')\n",
    "test  = pd.read_csv(DATA_DIR +'/test.csv')\n",
    "\n",
    "df = train"
   ],
   "id": "3c06050307a128f4",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T20:14:47.382266Z",
     "start_time": "2024-12-19T20:14:47.376247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inspect_original_hla_values(df):\n",
    "    hla_vars = [col for col in df.columns if 'hla' in col.lower()]\n",
    "    print(\"\\nOriginal HLA Values:\")\n",
    "    for col in hla_vars:\n",
    "        print(f\"\\n{col}\")\n",
    "        print(\"Unique values:\", sorted(df[col].dropna().unique()))\n",
    "        print(\"Value counts:\\n\", df[col].value_counts().head())\n",
    "        print(\"Missing:\", df[col].isna().sum())"
   ],
   "id": "b89acf0c7bc221be",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7d63dd54b50c5ea2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T20:15:24.219178Z",
     "start_time": "2024-12-19T20:15:24.178258Z"
    }
   },
   "cell_type": "code",
   "source": "inspect_original_hla_values(df)",
   "id": "79eb1488df0f28e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original HLA Values:\n",
      "\n",
      "hla_match_c_high\n",
      "Unique values: [0.0, 1.0, 2.0]\n",
      "Value counts:\n",
      " hla_match_c_high\n",
      "2.0    18565\n",
      "1.0     5536\n",
      "0.0       79\n",
      "Name: count, dtype: int64\n",
      "Missing: 4620\n",
      "\n",
      "hla_high_res_8\n",
      "Unique values: [2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "Value counts:\n",
      " hla_high_res_8\n",
      "8.0    13568\n",
      "4.0     3820\n",
      "7.0     2385\n",
      "5.0     1648\n",
      "6.0     1520\n",
      "Name: count, dtype: int64\n",
      "Missing: 5829\n",
      "\n",
      "hla_low_res_6\n",
      "Unique values: [2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "Value counts:\n",
      " hla_low_res_6\n",
      "6.0    15690\n",
      "3.0     4955\n",
      "5.0     2808\n",
      "4.0     2055\n",
      "2.0       22\n",
      "Name: count, dtype: int64\n",
      "Missing: 3270\n",
      "\n",
      "hla_high_res_6\n",
      "Unique values: [0.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "Value counts:\n",
      " hla_high_res_6\n",
      "6.0    14022\n",
      "3.0     4596\n",
      "5.0     2726\n",
      "4.0     2128\n",
      "2.0       43\n",
      "Name: count, dtype: int64\n",
      "Missing: 5284\n",
      "\n",
      "hla_high_res_10\n",
      "Unique values: [3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
      "Value counts:\n",
      " hla_high_res_10\n",
      "10.0    12232\n",
      "5.0      3161\n",
      "9.0      2369\n",
      "6.0      1355\n",
      "8.0      1314\n",
      "Name: count, dtype: int64\n",
      "Missing: 7163\n",
      "\n",
      "hla_match_dqb1_high\n",
      "Unique values: [0.0, 1.0, 2.0]\n",
      "Value counts:\n",
      " hla_match_dqb1_high\n",
      "2.0    17468\n",
      "1.0     6056\n",
      "0.0       77\n",
      "Name: count, dtype: int64\n",
      "Missing: 5199\n",
      "\n",
      "hla_nmdp_6\n",
      "Unique values: [2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "Value counts:\n",
      " hla_nmdp_6\n",
      "6.0    15105\n",
      "3.0     4888\n",
      "5.0     3296\n",
      "4.0     1279\n",
      "2.0       35\n",
      "Name: count, dtype: int64\n",
      "Missing: 4197\n",
      "\n",
      "hla_match_c_low\n",
      "Unique values: [0.0, 1.0, 2.0]\n",
      "Value counts:\n",
      " hla_match_c_low\n",
      "2.0    19782\n",
      "1.0     6139\n",
      "0.0       79\n",
      "Name: count, dtype: int64\n",
      "Missing: 2800\n",
      "\n",
      "hla_match_drb1_low\n",
      "Unique values: [1.0, 2.0]\n",
      "Value counts:\n",
      " hla_match_drb1_low\n",
      "2.0    18710\n",
      "1.0     7447\n",
      "Name: count, dtype: int64\n",
      "Missing: 2643\n",
      "\n",
      "hla_match_dqb1_low\n",
      "Unique values: [0.0, 1.0, 2.0]\n",
      "Value counts:\n",
      " hla_match_dqb1_low\n",
      "2.0    19131\n",
      "1.0     5384\n",
      "0.0       91\n",
      "Name: count, dtype: int64\n",
      "Missing: 4194\n",
      "\n",
      "hla_match_a_high\n",
      "Unique values: [0.0, 1.0, 2.0]\n",
      "Value counts:\n",
      " hla_match_a_high\n",
      "2.0    17304\n",
      "1.0     7132\n",
      "0.0       63\n",
      "Name: count, dtype: int64\n",
      "Missing: 4301\n",
      "\n",
      "hla_match_b_low\n",
      "Unique values: [0.0, 1.0, 2.0]\n",
      "Value counts:\n",
      " hla_match_b_low\n",
      "2.0    18951\n",
      "1.0     7220\n",
      "0.0       64\n",
      "Name: count, dtype: int64\n",
      "Missing: 2565\n",
      "\n",
      "hla_match_a_low\n",
      "Unique values: [0.0, 1.0, 2.0]\n",
      "Value counts:\n",
      " hla_match_a_low\n",
      "2.0    18776\n",
      "1.0     7585\n",
      "0.0       49\n",
      "Name: count, dtype: int64\n",
      "Missing: 2390\n",
      "\n",
      "hla_match_b_high\n",
      "Unique values: [0.0, 1.0, 2.0]\n",
      "Value counts:\n",
      " hla_match_b_high\n",
      "2.0    17366\n",
      "1.0     7269\n",
      "0.0       77\n",
      "Name: count, dtype: int64\n",
      "Missing: 4088\n",
      "\n",
      "hla_low_res_8\n",
      "Unique values: [2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "Value counts:\n",
      " hla_low_res_8\n",
      "8.0    15160\n",
      "4.0     4259\n",
      "7.0     2603\n",
      "5.0     1613\n",
      "6.0     1488\n",
      "Name: count, dtype: int64\n",
      "Missing: 3653\n",
      "\n",
      "hla_match_drb1_high\n",
      "Unique values: [0.0, 1.0, 2.0]\n",
      "Value counts:\n",
      " hla_match_drb1_high\n",
      "2.0    18066\n",
      "1.0     7311\n",
      "0.0       71\n",
      "Name: count, dtype: int64\n",
      "Missing: 3352\n",
      "\n",
      "hla_low_res_10\n",
      "Unique values: [4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
      "Value counts:\n",
      " hla_low_res_10\n",
      "10.0    13734\n",
      "5.0      3211\n",
      "9.0      2544\n",
      "6.0      1664\n",
      "8.0      1387\n",
      "Name: count, dtype: int64\n",
      "Missing: 5064\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T20:15:12.563458Z",
     "start_time": "2024-12-19T20:15:12.552007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def debug_hla_processing(df, hla_data, hla_imputed, hla_vars):\n",
    "    print(\"\\nData Type Changes:\")\n",
    "    for idx, col in enumerate(hla_vars):\n",
    "        print(f\"\\n{col}\")\n",
    "        print(f\"Original dtype: {df[col].dtype}\")\n",
    "        print(f\"Pre-MICE dtype: {hla_data[col].dtype}\")\n",
    "        print(f\"Post-MICE values range: {hla_imputed[:, idx].min()} to {hla_imputed[:, idx].max()}\")"
   ],
   "id": "23e3182b807a852c",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T20:25:50.184063Z",
     "start_time": "2024-12-19T20:25:49.612690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_column_with_value(df, value='No'):\n",
    "    for col in df.columns:\n",
    "        if df[col].astype(str).eq(value).any():\n",
    "            print(f\"Found '{value}' in column: {col}\")\n",
    "            print(\"Value counts:\\n\", df[col].value_counts())\n",
    "            print(\"Dtype:\", df[col].dtype)\n",
    "            print()\n",
    "find_column_with_value(df)"
   ],
   "id": "dfec265f080a9759",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'No' in column: psych_disturb\n",
      "Value counts:\n",
      " psych_disturb\n",
      "No          23005\n",
      "Yes          3587\n",
      "Not done      146\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: diabetes\n",
      "Value counts:\n",
      " diabetes\n",
      "No          22201\n",
      "Yes          4339\n",
      "Not done      141\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: arrhythmia\n",
      "Value counts:\n",
      " arrhythmia\n",
      "No          25203\n",
      "Yes          1277\n",
      "Not done      118\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: vent_hist\n",
      "Value counts:\n",
      " vent_hist\n",
      "No     27721\n",
      "Yes      820\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: renal_issue\n",
      "Value counts:\n",
      " renal_issue\n",
      "No          26548\n",
      "Yes           200\n",
      "Not done      137\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: pulm_severe\n",
      "Value counts:\n",
      " pulm_severe\n",
      "No          24779\n",
      "Yes          1706\n",
      "Not done      180\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: rituximab\n",
      "Value counts:\n",
      " rituximab\n",
      "No     26033\n",
      "Yes      619\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: obesity\n",
      "Value counts:\n",
      " obesity\n",
      "No          25144\n",
      "Yes          1779\n",
      "Not done      117\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: in_vivo_tcd\n",
      "Value counts:\n",
      " in_vivo_tcd\n",
      "No     17591\n",
      "Yes    10984\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: hepatic_severe\n",
      "Value counts:\n",
      " hepatic_severe\n",
      "No          25238\n",
      "Yes          1481\n",
      "Not done      210\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: prior_tumor\n",
      "Value counts:\n",
      " prior_tumor\n",
      "No          23828\n",
      "Yes          3009\n",
      "Not done      285\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: peptic_ulcer\n",
      "Value counts:\n",
      " peptic_ulcer\n",
      "No          25956\n",
      "Yes           259\n",
      "Not done      166\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: rheum_issue\n",
      "Value counts:\n",
      " rheum_issue\n",
      "No          26015\n",
      "Yes           457\n",
      "Not done      145\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: hepatic_mild\n",
      "Value counts:\n",
      " hepatic_mild\n",
      "No          24989\n",
      "Yes          1754\n",
      "Not done      140\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: cardiac\n",
      "Value counts:\n",
      " cardiac\n",
      "No          24592\n",
      "Yes          1519\n",
      "Not done      147\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n",
      "Found 'No' in column: pulm_moderate\n",
      "Value counts:\n",
      " pulm_moderate\n",
      "No          21338\n",
      "Yes          5249\n",
      "Not done      166\n",
      "Name: count, dtype: int64\n",
      "Dtype: object\n",
      "\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T21:18:30.479179Z",
     "start_time": "2024-12-19T21:18:30.427145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self):\n",
    "        # Existing initializations\n",
    "        \n",
    "        self.year_bins = None\n",
    "        self.karnofsky_bins = [0, 60, 75, 85, 100]\n",
    "        self.comorbidity_bins = [0, 2, 4, float('inf')]\n",
    "        self.age_bins = [0, 20, 40, 60, float('inf')]\n",
    "        self.risk_bins = [-float('inf'), -1, 1, float('inf')]\n",
    "        self.hla_bins = [-float('inf'), -1, 0, float('inf')]\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoders = {}\n",
    "        self.mice_imputer = None\n",
    "        self.knn_imputers = {}\n",
    "        self.fill_values = {}\n",
    "\n",
    "        # Add KM fitter\n",
    "        self.kmf = KaplanMeierFitter()\n",
    "\n",
    "        self.ordinal_encoders = {}\n",
    "        self.hla_imputers = {}\n",
    "        self.clinical_encoders = {}\n",
    "        self.disease_imputers = {}\n",
    "\n",
    "        self.categorical_mappings = {\n",
    "            'dri_score': ['N/A - non-malignant indication', 'Intermediate', 'High', 'Low',\n",
    "                          'N/A - disease not classifiable', 'N/A - pediatric', 'TBD cytogenetics',\n",
    "                          'Intermediate - TED AML case <missing cytogenetics>', 'Unknown',\n",
    "                          'High - TED AML case <missing cytogenetics>', 'Very high',\n",
    "                          'Missing disease status'],\n",
    "            'cyto_score': ['Unknown', 'Intermediate', 'Poor', 'Other', 'Favorable', 'TBD',\n",
    "                           'Normal', 'Not tested'],\n",
    "            'tbi_status': ['No TBI', 'TBI +- Other, >cGy', 'TBI + Cy +- Other',\n",
    "                           'TBI +- Other, <=cGy', 'TBI +- Other, unknown dose',\n",
    "                           'TBI +- Other, -cGy, fractionated', 'TBI +- Other, -cGy, single',\n",
    "                           'TBI +- Other, -cGy, unknown dose'],\n",
    "            'graft_type': ['Bone marrow', 'Peripheral blood'],\n",
    "            'vent_hist': ['No', 'Yes', 'Unknown'],\n",
    "            'prim_disease_hct': ['IEA', 'AML', 'HIS', 'ALL', 'MPN', 'IIS', 'Solid tumor',\n",
    "                                 'Other leukemia', 'PCD', 'IPA', 'IMD', 'MDS', 'NHL', 'SAA',\n",
    "                                 'AI', 'CML', 'Other acute leukemia', 'HD'],\n",
    "            'cmv_status': ['+/+', '-/+', '-/-', 'Unknown', '+/-'],\n",
    "            'tce_imm_match': ['Unknown', 'P/P', 'G/B', 'H/B', 'G/G', 'P/H', 'P/B', 'H/H', 'P/G'],\n",
    "            'rituximab': ['No', 'Unknown', 'Yes'],\n",
    "            'prod_type': ['BM', 'PB'],\n",
    "            'cyto_score_detail': ['Unknown', 'Intermediate', 'TBD', 'Poor', 'Favorable', 'Not tested'],\n",
    "            'conditioning_intensity': ['Unknown', 'MAC', 'RIC', 'NMA', 'TBD', 'No drugs reported',\n",
    "                                       'N/A, F(pre-TED) not submitted'],\n",
    "            'ethnicity': ['Not Hispanic or Latino', 'Hispanic or Latino', 'Non-resident of the U.S.'],\n",
    "            'mrd_hct': ['Unknown', 'Positive', 'Negative'],\n",
    "            'in_vivo_tcd': ['Yes', 'No', 'Unknown'],\n",
    "            'tce_match': ['Unknown', 'Permissive', 'HvG non-permissive', 'Fully matched',\n",
    "                          'GvH non-permissive'],\n",
    "            'gvhd_proph': ['FKalone', 'Other GVHD Prophylaxis', 'Cyclophosphamide alone',\n",
    "                           'FK+ MMF +- others', 'TDEPLETION +- other',\n",
    "                           'CSA + MMF +- others(not FK)', 'CSA + MTX +- others(not MMF,FK)',\n",
    "                           'FK+ MTX +- others(not MMF)', 'Cyclophosphamide +- others',\n",
    "                           'CSA alone', 'Unknown', 'TDEPLETION alone', 'No GvHD Prophylaxis',\n",
    "                           'CDselect alone', 'CDselect +- other', 'Parent Q = yes, but no agent',\n",
    "                           'FK+- others(not MMF,MTX)', 'CSA +- others(not FK,MMF,MTX)'],\n",
    "            'sex_match': ['M-F', 'F-F', 'F-M', 'M-M', 'Unknown'],\n",
    "            'race_group': ['More than one race', 'Asian', 'White',\n",
    "                           'American Indian or Alaska Native',\n",
    "                           'Native Hawaiian or other Pacific Islander',\n",
    "                           'Black or African-American'],\n",
    "            'tce_div_match': ['Unknown', 'Permissive mismatched', 'GvH non-permissive',\n",
    "                              'HvG non-permissive', 'Bi-directional non-permissive'],\n",
    "            'donor_related': ['Unrelated', 'Related', 'Multiple donor (non-UCB)', 'Unknown'],\n",
    "            'conditioning_karnofsky': ['Unknown_very_high', 'MAC_very_high', 'RIC_very_high',\n",
    "                                       'MAC_low', 'MAC_high', 'RIC_high', 'NMA_low',\n",
    "                                       'NMA_very_high', 'RIC_very_low', 'NMA_high',\n",
    "                                       'Unknown_low', 'Unknown_high', 'RIC_low', 'TBD_low',\n",
    "                                       'Unknown_very_low', 'TBD_very_high',\n",
    "                                       'No drugs reported_very_high', 'MAC_very_low',\n",
    "                                       'NMA_very_low', 'N/A, F(pre-TED) not submitted_very_high',\n",
    "                                       'No drugs reported_low', 'TBD_high', 'TBD_very_low',\n",
    "                                       'N/A, F(pre-TED) not submitted_high',\n",
    "                                       'N/A, F(pre-TED) not submitted_low',\n",
    "                                       'No drugs reported_very_low',\n",
    "                                       'N/A, F(pre-TED) not submitted_very_low',\n",
    "                                       'No drugs reported_high'],\n",
    "            'transplant_era': ['very_early', 'early', 'mid', 'late', 'recent'],\n",
    "            'era_conditioning': ['early_Unknown', 'very_early_MAC', 'recent_Unknown',\n",
    "                                 'late_MAC', 'early_MAC', 'early_RIC', 'late_Unknown',\n",
    "                                 'mid_MAC', 'very_early_Unknown', 'very_early_RIC',\n",
    "                                 'late_NMA', 'very_early_NMA', 'mid_NMA', 'mid_RIC',\n",
    "                                 'mid_Unknown', 'late_RIC', 'recent_RIC', 'early_TBD',\n",
    "                                 'recent_NMA', 'recent_TBD', 'early_No drugs reported',\n",
    "                                 'early_NMA', 'recent_MAC', 'very_early_TBD',\n",
    "                                 'very_early_N/A, F(pre-TED) not submitted',\n",
    "                                 'very_early_No drugs reported',\n",
    "                                 'late_N/A, F(pre-TED) not submitted', 'late_TBD',\n",
    "                                 'mid_No drugs reported',\n",
    "                                 'mid_N/A, F(pre-TED) not submitted', 'late_No drugs reported',\n",
    "                                 'mid_TBD', 'early_N/A, F(pre-TED) not submitted',\n",
    "                                 'recent_No drugs reported',\n",
    "                                 'recent_N/A, F(pre-TED) not submitted'],\n",
    "            'gvhd_main_strategy': ['FK_based', 'other', 'Cy_based', 'T_depletion',\n",
    "                                   'CSA_based', None, 'none', 'CD_selection']\n",
    "        }\n",
    "\n",
    "        # Initialize label encoders\n",
    "        self.initialize_label_encoders()\n",
    "\n",
    "\n",
    "        # Create and fit label encoders for each categorical variable\n",
    "        for var, categories in self.categorical_mappings.items():\n",
    "            encoder = LabelEncoder()\n",
    "            # Add 'Unknown' if not present\n",
    "            if 'Unknown' not in categories:\n",
    "                categories = list(categories) + ['Unknown']\n",
    "            encoder.fit(categories)\n",
    "            self.label_encoders[var] = encoder\n",
    "\n",
    "    def initialize_label_encoders(self):\n",
    "        # Use the instance variable self.categorical_mappings\n",
    "        self.label_encoders = {}\n",
    "        for col, values in self.categorical_mappings.items():\n",
    "            le = LabelEncoder()\n",
    "            values = [str(v) if v is None else v for v in values]\n",
    "            le.fit(values)\n",
    "            self.label_encoders[col] = le\n",
    "\n",
    "    def transform_survival_probability(self, df, time_col='efs_time', event_col='efs', training=False):\n",
    "        \"\"\"\n",
    "        Transform using survival probability estimates\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            # Fit KM model on training data\n",
    "            self.kmf.fit(df[time_col], df[event_col])\n",
    "\n",
    "        # Transform both training and test data\n",
    "        y = self.kmf.survival_function_at_times(df[time_col]).values\n",
    "\n",
    "        # Adjust probabilities for censored cases only in training\n",
    "        if training:\n",
    "            y[df[event_col] == 0] -= 0.2\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "# why are trainnig handled differently to validation?\n",
    "    # why are all categorical handled before the others...\n",
    "# why are there suddenly extra columns??\n",
    "    def process_missing_data_enhanced(self, df, training):\n",
    "        \"\"\"\n",
    "        Enhanced missing data handling treating HLA variables as ordinal categories.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            training: Boolean indicating if this is training data\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with processed and imputed values\n",
    "        \"\"\"\n",
    "        df_processed = df.copy()\n",
    "\n",
    "        # 1. First identify all variables\n",
    "        hla_vars = [col for col in df_processed.columns if 'hla' in col.lower()]\n",
    "        categorical_vars = [\n",
    "            'tce_match', 'mrd_hct', 'cyto_score_detail', 'tce_div_match',\n",
    "            'tce_imm_match', 'cyto_score', 'cmv_status', 'dri_score',\n",
    "            'conditioning_intensity', 'rituximab', 'in_vivo_tcd',\n",
    "            'gvhd_proph', 'sex_match', 'donor_related', 'vent_hist'\n",
    "        ]\n",
    "        clinical_vars = [\n",
    "            'cardiac', 'arrhythmia', 'pulm_severe', 'pulm_moderate',\n",
    "            'hepatic_mild', 'hepatic_severe', 'renal_issue', 'diabetes',\n",
    "            'psych_disturb', 'peptic_ulcer', 'rheum_issue', 'obesity',\n",
    "            'prior_tumor', 'melphalan_dose'\n",
    "        ]\n",
    "\n",
    "        # 2. Create missingness indicators for HLA variables\n",
    "        for col in hla_vars:\n",
    "            df_processed[f'{col}_missing'] = df_processed[col].isna().astype(int)\n",
    "\n",
    "        df_processed['hla_testing_completeness'] = df_processed[hla_vars].notna().sum(axis=1) / len(hla_vars)\n",
    "\n",
    "        # 3. Handle HLA variables as ordinal categories\n",
    "        for col in hla_vars:\n",
    "            if col in df_processed.columns:\n",
    "                if training:\n",
    "                    # Get valid values for this column\n",
    "                    valid_values = sorted(df_processed[col].dropna().unique())\n",
    "\n",
    "                    # Create and fit imputer first\n",
    "                    self.hla_imputers[col] = SimpleImputer(strategy='most_frequent')\n",
    "                    self.hla_imputers[col].fit(df_processed[[col]])\n",
    "\n",
    "                    # Create ordinal encoder that handles unknown values\n",
    "                    self.ordinal_encoders[col] = OrdinalEncoder(\n",
    "                        categories=[valid_values],\n",
    "                        dtype=int,\n",
    "                        handle_unknown='use_encoded_value',\n",
    "                        unknown_value=-1\n",
    "                    )\n",
    "\n",
    "                    # First impute the values\n",
    "                    df_processed[col] = self.hla_imputers[col].transform(df_processed[[col]])\n",
    "\n",
    "                    # Then fit the encoder on the imputed values\n",
    "                    self.ordinal_encoders[col].fit(df_processed[col].values.reshape(-1, 1))\n",
    "                else:\n",
    "                    # For inference, first impute then encode\n",
    "                    df_processed[col] = self.hla_imputers[col].transform(df_processed[[col]])\n",
    "\n",
    "                # Finally encode all values\n",
    "                df_processed[col] = self.ordinal_encoders[col].transform(\n",
    "                    df_processed[col].values.reshape(-1, 1)\n",
    "                ).ravel()\n",
    "\n",
    "        # 4. Handle categorical variables\n",
    "        for var in categorical_vars:\n",
    "            if var in df_processed.columns:\n",
    "                if training:\n",
    "                    # Store original categories and add 'Unknown' for future unseen values\n",
    "                    unique_vals = set(df_processed[var].dropna().unique())\n",
    "                    unique_vals.add('Unknown')\n",
    "                    self.unique_categories = getattr(self, 'unique_categories', {})\n",
    "                    self.unique_categories[var] = unique_vals\n",
    "\n",
    "                    # Fill NA with 'Unknown'\n",
    "                    df_processed[var] = df_processed[var].fillna('Unknown')\n",
    "                    self.label_encoders[var] = LabelEncoder()\n",
    "                    df_processed[var] = self.label_encoders[var].fit_transform(df_processed[var])\n",
    "                else:\n",
    "                    if var in self.label_encoders:\n",
    "                        # Replace unseen categories with 'Unknown'\n",
    "                        df_processed[var] = df_processed[var].fillna('Unknown')\n",
    "                        df_processed[var] = df_processed[var].map(\n",
    "                            lambda x: 'Unknown' if x not in self.unique_categories[var] else x\n",
    "                        )\n",
    "                        df_processed[var] = self.label_encoders[var].transform(df_processed[var])\n",
    "\n",
    "        # 5. Handle clinical variables by disease group\n",
    "        remaining_clinical = [col for col in df_processed.columns\n",
    "                              if col not in hla_vars + categorical_vars + clinical_vars +\n",
    "                              ['prim_disease_hct'] +\n",
    "                              [col + '_missing' for col in hla_vars] +\n",
    "                              ['hla_testing_completeness']]\n",
    "\n",
    "        # First encode clinical variables\n",
    "        for col in clinical_vars + remaining_clinical:\n",
    "            if df_processed[col].dtype == 'object' or df_processed[col].apply(lambda x: isinstance(x, str)).any():\n",
    "                if training:\n",
    "                    self.clinical_encoders[col] = LabelEncoder()\n",
    "                    non_null_mask = df_processed[col].notna()\n",
    "                    df_processed.loc[non_null_mask, col] = self.clinical_encoders[col].fit_transform(\n",
    "                        df_processed.loc[non_null_mask, col]\n",
    "                    )\n",
    "                else:\n",
    "                    if col in self.clinical_encoders:\n",
    "                        non_null_mask = df_processed[col].notna()\n",
    "                        df_processed.loc[non_null_mask, col] = self.clinical_encoders[col].transform(\n",
    "                            df_processed.loc[non_null_mask, col]\n",
    "                        )\n",
    "\n",
    "        # Now proceed with KNN imputation by disease group\n",
    "        for disease in df_processed['prim_disease_hct'].unique():\n",
    "            mask = df_processed['prim_disease_hct'] == disease\n",
    "            if mask.sum() > 0:\n",
    "                clinical_data = df_processed.loc[mask, clinical_vars + remaining_clinical].copy()\n",
    "\n",
    "                if training:\n",
    "                    n_neighbors = min(5, mask.sum() - 1)\n",
    "                    self.disease_imputers[disease] = KNNImputer(n_neighbors=n_neighbors)\n",
    "                    imputed_values = self.disease_imputers[disease].fit_transform(clinical_data)\n",
    "                else:\n",
    "                    if disease in self.disease_imputers:\n",
    "                        imputed_values = self.disease_imputers[disease].transform(clinical_data)\n",
    "                    else:\n",
    "                        # Fallback for unseen diseases\n",
    "                        for col in clinical_vars + remaining_clinical:\n",
    "                            clinical_data[col] = clinical_data[col].fillna(clinical_data[col].median())\n",
    "                        imputed_values = clinical_data.values\n",
    "\n",
    "                df_processed.loc[mask, clinical_vars + remaining_clinical] = imputed_values\n",
    "\n",
    "        # Convert encoded clinical variables back to original format\n",
    "        for col in clinical_vars + remaining_clinical:\n",
    "            if col in self.clinical_encoders:\n",
    "                df_processed[col] = self.clinical_encoders[col].inverse_transform(\n",
    "                    df_processed[col].astype(int)\n",
    "                )\n",
    "\n",
    "        return df_processed \n",
    "    \n",
    "    \n",
    "    def create_enhanced_features(self, df, training=False):\n",
    "        \"\"\"\n",
    "        Create enhanced feature set without using time information\n",
    "        \"\"\"\n",
    "        df_features = df.copy()\n",
    "    \n",
    "        # 1. Treatment-Patient Interactions\n",
    "        karnofsky_bins = [0, 60, 75, 85, 100]\n",
    "        karnofsky_cut = pd.cut(df_features['karnofsky_score'],\n",
    "                               bins=karnofsky_bins,\n",
    "                               labels=['very_low', 'low', 'high', 'very_high'],\n",
    "                               include_lowest=True)\n",
    "        df_features['conditioning_karnofsky'] = (\n",
    "                df_features['conditioning_intensity'].astype(str) + '_' +\n",
    "                karnofsky_cut.astype(str)\n",
    "        )\n",
    "    \n",
    "        comorbidity_bins = [0, 2, 4, float('inf')]\n",
    "        comorbidity_cut = pd.cut(df_features['comorbidity_score'],\n",
    "                                 bins=comorbidity_bins,\n",
    "                                 labels=['low', 'medium', 'high'],\n",
    "                                 include_lowest=True)\n",
    "        df_features['dri_comorbidity'] = (\n",
    "                df_features['dri_score'].astype(str) + '_' +\n",
    "                comorbidity_cut.astype(str)\n",
    "        )\n",
    "    \n",
    "        age_bins = [0, 20, 40, 60, float('inf')]\n",
    "        age_cut = pd.cut(df_features['age_at_hct'],\n",
    "                         bins=age_bins,\n",
    "                         labels=['young', 'mid', 'senior', 'elderly'],\n",
    "                         include_lowest=True)\n",
    "        df_features['age_conditioning'] = (\n",
    "                age_cut.astype(str) + '_' +\n",
    "                df_features['conditioning_intensity'].astype(str)\n",
    "        )\n",
    "    \n",
    "        # 2. HLA Matching Features\n",
    "        high_res_cols = ['hla_match_drb1_high', 'hla_match_dqb1_high', 'hla_match_c_high',\n",
    "                         'hla_match_a_high', 'hla_match_b_high']\n",
    "        low_res_cols = ['hla_match_drb1_low', 'hla_match_dqb1_low', 'hla_match_c_low',\n",
    "                        'hla_match_a_low', 'hla_match_b_low']\n",
    "    \n",
    "        df_features['hla_high_res_composite'] = df_features[high_res_cols].mean(axis=1)\n",
    "        df_features['hla_low_res_composite'] = df_features[low_res_cols].mean(axis=1)\n",
    "    \n",
    "        # 3. Composite Risk Scores\n",
    "        def standardize(x):\n",
    "            return (x - x.mean()) / x.std()\n",
    "    \n",
    "        df_features['patient_risk_score'] = (\n",
    "                standardize(df_features['age_at_hct']) +\n",
    "                standardize(df_features['comorbidity_score']) -\n",
    "                standardize(df_features['karnofsky_score'])\n",
    "        )\n",
    "    \n",
    "        risk_bins = [-float('inf'), -1, 1, float('inf')]\n",
    "        risk_cut = pd.cut(df_features['patient_risk_score'],\n",
    "                          bins=risk_bins,\n",
    "                          labels=['low', 'medium', 'high'])\n",
    "        df_features['risk_treatment_match'] = (\n",
    "                risk_cut.astype(str) + '_' +\n",
    "                df_features['conditioning_intensity'].astype(str)\n",
    "        )\n",
    "    \n",
    "        # 4. Era-based features\n",
    "        year_bins = df_features['year_hct'].quantile([0, 0.2, 0.4, 0.6, 0.8, 1.0]).values\n",
    "        df_features['transplant_era'] = pd.cut(df_features['year_hct'],\n",
    "                                               bins=year_bins,\n",
    "                                               labels=['very_early', 'early', 'mid', 'late', 'recent'],\n",
    "                                               include_lowest=True)\n",
    "    \n",
    "        df_features['era_conditioning'] = (\n",
    "                df_features['transplant_era'].astype(str) + '_' +\n",
    "                df_features['conditioning_intensity'].astype(str)\n",
    "        )\n",
    "    \n",
    "        hla_bins = [-float('inf'), -1, 0, float('inf')]\n",
    "        hla_cut = pd.cut(standardize(df_features['hla_high_res_composite']),\n",
    "                         bins=hla_bins,\n",
    "                         labels=['low', 'medium', 'high'])\n",
    "        df_features['era_hla_quality'] = (\n",
    "                df_features['transplant_era'].astype(str) + '_' +\n",
    "                hla_cut.astype(str)\n",
    "        )\n",
    "    \n",
    "        # 5. GVHD Prophylaxis Grouping\n",
    "        gvhd_mapping = {\n",
    "            'FK+ MMF +- others': 'FK_based',\n",
    "            'FK+ MTX +- others(not MMF)': 'FK_based',\n",
    "            'FKalone': 'FK_based',\n",
    "            'FK+- others(not MMF,MTX)': 'FK_based',\n",
    "            'CSA + MMF +- others(not FK)': 'CSA_based',\n",
    "            'CSA + MTX +- others(not MMF,FK)': 'CSA_based',\n",
    "            'CSA alone': 'CSA_based',\n",
    "            'CSA +- others(not FK,MMF,MTX)': 'CSA_based',\n",
    "            'TDEPLETION +- other': 'T_depletion',\n",
    "            'TDEPLETION alone': 'T_depletion',\n",
    "            'Cyclophosphamide alone': 'Cy_based',\n",
    "            'Cyclophosphamide +- others': 'Cy_based',\n",
    "            'CDselect alone': 'CD_selection',\n",
    "            'CDselect +- other': 'CD_selection',\n",
    "            'No GvHD Prophylaxis': 'none',\n",
    "            'Other GVHD Prophylaxis': 'other'\n",
    "        }\n",
    "        df_features['gvhd_main_strategy'] = df_features['gvhd_proph'].map(gvhd_mapping)\n",
    "    \n",
    "        # 6. Disease-specific Features\n",
    "        df_features['disease_conditioning'] = (\n",
    "                df_features['prim_disease_hct'].astype(str) + '_' +\n",
    "                df_features['conditioning_intensity'].astype(str)\n",
    "        )\n",
    "    \n",
    "        df_features['disease_era'] = (\n",
    "                df_features['prim_disease_hct'].astype(str) + '_' +\n",
    "                df_features['transplant_era'].astype(str)\n",
    "        )\n",
    "    \n",
    "        # 7. Enhanced HLA Features\n",
    "        df_features['hla_mismatch_pattern'] = (\n",
    "            (df_features['hla_high_res_composite'] < df_features['hla_low_res_composite']).astype(int)\n",
    "        )\n",
    "    \n",
    "        df_features['hla_era_trend'] = df_features.groupby('transplant_era')['hla_high_res_composite'].transform('mean')\n",
    "    \n",
    "        # 8. Comorbidity Interactions\n",
    "        comorbidity_cols = ['cardiac', 'pulm_severe', 'renal_issue', 'hepatic_severe']\n",
    "        df_features['comorbidity_pattern'] = df_features[comorbidity_cols].apply(\n",
    "            lambda x: '_'.join(x.index[x == 'Yes']), axis=1\n",
    "        )\n",
    "    \n",
    "        df_features['age_comorbidity_risk'] = df_features['age_at_hct'] * df_features['comorbidity_score']\n",
    "    \n",
    "        return df_features\n",
    "\n",
    "    def process_data(self, df, training=False):\n",
    "        \"\"\"\n",
    "        Main processing pipeline without fold creation\n",
    "        \"\"\"\n",
    "        df_processed = df.copy()\n",
    "\n",
    "        # Handle missing data\n",
    "        df_processed = self.process_missing_data_enhanced(df_processed, training)\n",
    "\n",
    "        # Create enhanced features\n",
    "        #df_processed = self.create_enhanced_features(df_processed, training)\n",
    "\n",
    "        # Transform survival probabilities and create labels\n",
    "        if 'efs_time' in df_processed.columns and 'efs' in df_processed.columns:\n",
    "            df_processed['label'] = self.transform_survival_probability(\n",
    "                df_processed,\n",
    "                time_col='efs_time',\n",
    "                event_col='efs',\n",
    "                training=training\n",
    "            )\n",
    "\n",
    "        return df_processed\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2c57ffaea646e778",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T21:00:01.223132Z",
     "start_time": "2024-12-19T20:59:56.976846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline = DataPipeline()\n",
    "\n",
    "\n",
    "\n",
    "# Process each fold's data first\n",
    "x_train = pipeline.process_missing_data_enhanced(train, training=True)\n"
   ],
   "id": "8641a8fe0c76c2fc",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T21:00:11.340734Z",
     "start_time": "2024-12-19T21:00:11.338319Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2700e9d587d13394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T20:33:43.915491Z",
     "start_time": "2024-12-19T20:33:43.907882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_processed_values(df):\n",
    "    \"\"\"\n",
    "    Validates processed dataframe for suspicious or invalid values.\n",
    "    Prints validation results.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Starting Validation ===\")\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    print(f\"\\nFound {len(numeric_cols)} numeric columns\")\n",
    "\n",
    "    suspicious_columns = []\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        print(f\"\\n{'-'*40}\")\n",
    "        print(f\"Column: {col}\")\n",
    "        print(f\"{'-'*40}\")\n",
    "\n",
    "        values = df[col].values\n",
    "\n",
    "        # Basic stats\n",
    "        stats = {\n",
    "            'min': values.min(),\n",
    "            'max': values.max(),\n",
    "            'mean': values.mean(),\n",
    "            'median': np.median(values),\n",
    "            'std': values.std(),\n",
    "            'zeros_pct': (values == 0).mean() * 100\n",
    "        }\n",
    "\n",
    "        for stat, value in stats.items():\n",
    "            print(f\"{stat:<10}: {value:,.4f}\")\n",
    "\n",
    "        # Check for issues\n",
    "        if np.any(np.isinf(values)):\n",
    "            suspicious_columns.append(f\"{col} (inf values)\")\n",
    "            print(\"WARNING: Found infinite values\")\n",
    "\n",
    "        abs_max = np.abs(values).max()\n",
    "        if abs_max > 1e10:\n",
    "            suspicious_columns.append(f\"{col} (large values: {abs_max})\")\n",
    "            print(f\"WARNING: Found very large values: {abs_max}\")\n",
    "\n",
    "        # Check for scientific notation\n",
    "        if df[col].astype(str).str.contains('e', case=False).any():\n",
    "            suspicious_columns.append(f\"{col} (scientific notation)\")\n",
    "            print(\"WARNING: Found scientific notation values\")\n",
    "\n",
    "        # Check for very small non-zero values\n",
    "        non_zero_vals = values[values != 0]\n",
    "        if len(non_zero_vals) > 0:\n",
    "            min_non_zero = np.abs(non_zero_vals).min()\n",
    "            if min_non_zero < 1e-10:\n",
    "                print(f\"WARNING: Found very small non-zero values: {min_non_zero}\")\n",
    "\n",
    "    if suspicious_columns:\n",
    "        print(\"\\n=== WARNINGS ===\")\n",
    "        print(\"Found suspicious values in these columns:\")\n",
    "        for col in suspicious_columns:\n",
    "            print(f\"- {col}\")\n",
    "    else:\n",
    "        print(\"\\n=== NO WARNINGS ===\")\n",
    "        print(\"All numeric columns appear to have reasonable values\")"
   ],
   "id": "96be47e9b01768cf",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d9f41d6a1b825056"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T21:42:22.637565Z",
     "start_time": "2024-12-19T21:42:21.567695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# After preprocessing\n",
    "#df_processed = pipeline.process_missing_data_enhanced(df, training=True)\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_processed_values(x_train)\n",
    "\n",
    "\n"
   ],
   "id": "e85f6cb478766d08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Validation ===\n",
      "\n",
      "Found 61 numeric columns\n",
      "\n",
      "----------------------------------------\n",
      "Column: dri_score\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 11.0000\n",
      "mean      : 3.8267\n",
      "median    : 2.0000\n",
      "std       : 3.1955\n",
      "zeros_pct : 16.2977\n",
      "\n",
      "----------------------------------------\n",
      "Column: cyto_score\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 7.0000\n",
      "mean      : 4.1073\n",
      "median    : 5.0000\n",
      "std       : 2.6032\n",
      "zeros_pct : 10.6163\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_c_high\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 1.8013\n",
      "median    : 2.0000\n",
      "std       : 0.4056\n",
      "zeros_pct : 0.2648\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_high_res_8\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 6.0000\n",
      "mean      : 5.0988\n",
      "median    : 6.0000\n",
      "std       : 1.4706\n",
      "zeros_pct : 0.0043\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_low_res_6\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 4.0000\n",
      "mean      : 3.2352\n",
      "median    : 4.0000\n",
      "std       : 1.1719\n",
      "zeros_pct : 0.0738\n",
      "\n",
      "----------------------------------------\n",
      "Column: vent_hist\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 0.0665\n",
      "median    : 0.0000\n",
      "std       : 0.3461\n",
      "zeros_pct : 96.2370\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_high_res_6\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 4.0000\n",
      "mean      : 3.2676\n",
      "median    : 4.0000\n",
      "std       : 1.1518\n",
      "zeros_pct : 0.1345\n",
      "\n",
      "----------------------------------------\n",
      "Column: cmv_status\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 4.0000\n",
      "mean      : 1.0855\n",
      "median    : 1.0000\n",
      "std       : 1.1819\n",
      "zeros_pct : 47.0747\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_high_res_10\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 7.0000\n",
      "mean      : 5.9542\n",
      "median    : 7.0000\n",
      "std       : 1.7598\n",
      "zeros_pct : 0.0043\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_dqb1_high\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 1.7829\n",
      "median    : 2.0000\n",
      "std       : 0.4181\n",
      "zeros_pct : 0.2431\n",
      "\n",
      "----------------------------------------\n",
      "Column: tce_imm_match\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 8.0000\n",
      "mean      : 6.5279\n",
      "median    : 7.0000\n",
      "std       : 2.2533\n",
      "zeros_pct : 1.8707\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_nmdp_6\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 4.0000\n",
      "mean      : 3.2784\n",
      "median    : 4.0000\n",
      "std       : 1.1530\n",
      "zeros_pct : 0.1042\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_c_low\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 1.7801\n",
      "median    : 2.0000\n",
      "std       : 0.4209\n",
      "zeros_pct : 0.2821\n",
      "\n",
      "----------------------------------------\n",
      "Column: rituximab\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 0.1173\n",
      "median    : 0.0000\n",
      "std       : 0.3824\n",
      "zeros_pct : 90.4080\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_drb1_low\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.7405\n",
      "median    : 1.0000\n",
      "std       : 0.4384\n",
      "zeros_pct : 25.9505\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_dqb1_low\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 1.8071\n",
      "median    : 2.0000\n",
      "std       : 0.4024\n",
      "zeros_pct : 0.3125\n",
      "\n",
      "----------------------------------------\n",
      "Column: cyto_score_detail\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 5.0000\n",
      "mean      : 2.9569\n",
      "median    : 3.0000\n",
      "std       : 1.9046\n",
      "zeros_pct : 4.2708\n",
      "\n",
      "----------------------------------------\n",
      "Column: conditioning_intensity\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 6.0000\n",
      "mean      : 2.3915\n",
      "median    : 2.0000\n",
      "std       : 2.3284\n",
      "zeros_pct : 42.6519\n",
      "\n",
      "----------------------------------------\n",
      "Column: year_hct\n",
      "----------------------------------------\n",
      "min       : 2,008.0000\n",
      "max       : 2,020.0000\n",
      "mean      : 2,015.1625\n",
      "median    : 2,016.0000\n",
      "std       : 3.1626\n",
      "zeros_pct : 0.0000\n",
      "\n",
      "----------------------------------------\n",
      "Column: mrd_hct\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 1.3006\n",
      "median    : 2.0000\n",
      "std       : 0.8754\n",
      "zeros_pct : 27.8082\n",
      "\n",
      "----------------------------------------\n",
      "Column: in_vivo_tcd\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 0.7712\n",
      "median    : 0.0000\n",
      "std       : 0.9694\n",
      "zeros_pct : 61.0417\n",
      "\n",
      "----------------------------------------\n",
      "Column: tce_match\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 4.0000\n",
      "mean      : 3.4077\n",
      "median    : 4.0000\n",
      "std       : 1.0375\n",
      "zeros_pct : 3.6545\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_a_high\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 1.7467\n",
      "median    : 2.0000\n",
      "std       : 0.4394\n",
      "zeros_pct : 0.1997\n",
      "\n",
      "----------------------------------------\n",
      "Column: donor_age\n",
      "----------------------------------------\n",
      "min       : 18.0100\n",
      "max       : 84.8000\n",
      "mean      : 42.4099\n",
      "median    : 40.1216\n",
      "std       : 14.8721\n",
      "zeros_pct : 0.0000\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_b_low\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 1.7412\n",
      "median    : 2.0000\n",
      "std       : 0.4428\n",
      "zeros_pct : 0.2127\n",
      "\n",
      "----------------------------------------\n",
      "Column: age_at_hct\n",
      "----------------------------------------\n",
      "min       : 0.0440\n",
      "max       : 73.7260\n",
      "mean      : 38.5436\n",
      "median    : 40.8990\n",
      "std       : 21.1854\n",
      "zeros_pct : 0.0000\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_a_low\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 1.7320\n",
      "median    : 2.0000\n",
      "std       : 0.4466\n",
      "zeros_pct : 0.1649\n",
      "\n",
      "----------------------------------------\n",
      "Column: gvhd_proph\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 17.0000\n",
      "mean      : 7.7987\n",
      "median    : 8.0000\n",
      "std       : 2.8506\n",
      "zeros_pct : 0.2040\n",
      "\n",
      "----------------------------------------\n",
      "Column: sex_match\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 4.0000\n",
      "mean      : 1.6103\n",
      "median    : 2.0000\n",
      "std       : 1.1248\n",
      "zeros_pct : 20.7595\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_b_high\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 1.7404\n",
      "median    : 2.0000\n",
      "std       : 0.4444\n",
      "zeros_pct : 0.2648\n",
      "\n",
      "----------------------------------------\n",
      "Column: comorbidity_score\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 10.0000\n",
      "mean      : 1.6953\n",
      "median    : 1.0000\n",
      "std       : 1.9721\n",
      "zeros_pct : 37.3481\n",
      "\n",
      "----------------------------------------\n",
      "Column: karnofsky_score\n",
      "----------------------------------------\n",
      "min       : 40.0000\n",
      "max       : 100.0000\n",
      "mean      : 83.8244\n",
      "median    : 90.0000\n",
      "std       : 10.9205\n",
      "zeros_pct : 0.0000\n",
      "\n",
      "----------------------------------------\n",
      "Column: tce_div_match\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 4.0000\n",
      "mean      : 3.1159\n",
      "median    : 3.0000\n",
      "std       : 0.9782\n",
      "zeros_pct : 2.0139\n",
      "\n",
      "----------------------------------------\n",
      "Column: donor_related\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 3.0000\n",
      "mean      : 1.8329\n",
      "median    : 1.0000\n",
      "std       : 1.0014\n",
      "zeros_pct : 1.2153\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_low_res_8\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 6.0000\n",
      "mean      : 5.0359\n",
      "median    : 6.0000\n",
      "std       : 1.5101\n",
      "zeros_pct : 0.0043\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_drb1_high\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 2.0000\n",
      "mean      : 1.7393\n",
      "median    : 2.0000\n",
      "std       : 0.4448\n",
      "zeros_pct : 0.2561\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_low_res_10\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 6.0000\n",
      "mean      : 4.8926\n",
      "median    : 6.0000\n",
      "std       : 1.7851\n",
      "zeros_pct : 0.0911\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_c_high_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1597\n",
      "median    : 0.0000\n",
      "std       : 0.3663\n",
      "zeros_pct : 84.0321\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_high_res_8_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.2017\n",
      "median    : 0.0000\n",
      "std       : 0.4013\n",
      "zeros_pct : 79.8264\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_low_res_6_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1133\n",
      "median    : 0.0000\n",
      "std       : 0.3170\n",
      "zeros_pct : 88.6675\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_high_res_6_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1823\n",
      "median    : 0.0000\n",
      "std       : 0.3861\n",
      "zeros_pct : 81.7708\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_high_res_10_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.2487\n",
      "median    : 0.0000\n",
      "std       : 0.4322\n",
      "zeros_pct : 75.1345\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_dqb1_high_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1800\n",
      "median    : 0.0000\n",
      "std       : 0.3842\n",
      "zeros_pct : 81.9965\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_nmdp_6_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1456\n",
      "median    : 0.0000\n",
      "std       : 0.3527\n",
      "zeros_pct : 85.4384\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_c_low_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.0964\n",
      "median    : 0.0000\n",
      "std       : 0.2951\n",
      "zeros_pct : 90.3646\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_drb1_low_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.0914\n",
      "median    : 0.0000\n",
      "std       : 0.2882\n",
      "zeros_pct : 90.8594\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_dqb1_low_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1449\n",
      "median    : 0.0000\n",
      "std       : 0.3520\n",
      "zeros_pct : 85.5122\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_a_high_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1480\n",
      "median    : 0.0000\n",
      "std       : 0.3551\n",
      "zeros_pct : 85.2040\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_b_low_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.0882\n",
      "median    : 0.0000\n",
      "std       : 0.2835\n",
      "zeros_pct : 91.1849\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_a_low_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.0819\n",
      "median    : 0.0000\n",
      "std       : 0.2741\n",
      "zeros_pct : 91.8142\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_b_high_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1408\n",
      "median    : 0.0000\n",
      "std       : 0.3478\n",
      "zeros_pct : 85.9201\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_low_res_8_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1266\n",
      "median    : 0.0000\n",
      "std       : 0.3325\n",
      "zeros_pct : 87.3438\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_match_drb1_high_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1148\n",
      "median    : 0.0000\n",
      "std       : 0.3188\n",
      "zeros_pct : 88.5156\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_low_res_10_missing\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.1751\n",
      "median    : 0.0000\n",
      "std       : 0.3801\n",
      "zeros_pct : 82.4870\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_testing_completeness\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.8565\n",
      "median    : 1.0000\n",
      "std       : 0.2913\n",
      "zeros_pct : 4.5877\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_high_res_composite\n",
      "----------------------------------------\n",
      "min       : 0.6000\n",
      "max       : 2.0000\n",
      "mean      : 1.7621\n",
      "median    : 2.0000\n",
      "std       : 0.3552\n",
      "zeros_pct : 0.0000\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_low_res_composite\n",
      "----------------------------------------\n",
      "min       : 0.6000\n",
      "max       : 1.8000\n",
      "mean      : 1.5602\n",
      "median    : 1.8000\n",
      "std       : 0.3585\n",
      "zeros_pct : 0.0000\n",
      "\n",
      "----------------------------------------\n",
      "Column: patient_risk_score\n",
      "----------------------------------------\n",
      "min       : -4.1580\n",
      "max       : 7.6966\n",
      "mean      : 0.0000\n",
      "median    : -0.1636\n",
      "std       : 2.0553\n",
      "zeros_pct : 0.0000\n",
      "WARNING: Found scientific notation values\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_mismatch_pattern\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 1.0000\n",
      "mean      : 0.0358\n",
      "median    : 0.0000\n",
      "std       : 0.1857\n",
      "zeros_pct : 96.4236\n",
      "\n",
      "----------------------------------------\n",
      "Column: hla_era_trend\n",
      "----------------------------------------\n",
      "min       : 1.6595\n",
      "max       : 1.8447\n",
      "mean      : 1.7621\n",
      "median    : 1.7840\n",
      "std       : 0.0620\n",
      "zeros_pct : 0.0000\n",
      "\n",
      "----------------------------------------\n",
      "Column: age_comorbidity_risk\n",
      "----------------------------------------\n",
      "min       : 0.0000\n",
      "max       : 711.1800\n",
      "mean      : 75.2020\n",
      "median    : 35.3880\n",
      "std       : 108.5069\n",
      "zeros_pct : 37.3481\n",
      "\n",
      "=== WARNINGS ===\n",
      "Found suspicious values in these columns:\n",
      "- patient_risk_score (scientific notation)\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load data\n",
    "# 2. Create temporary pipeline to analyze all categories\n",
    "#temp_pipeline = DataPipeline()\n",
    "#full_processed = temp_pipeline.process_data(train, training=True)\n",
    "\n",
    "# Print all categorical columns and their values\n",
    "#print(\"\\nAll categorical columns and their values:\")\n",
    "#for col in full_processed.select_dtypes(include=['object', 'category']).columns:\n",
    "#    print(f\"\\n{col}:\")\n",
    "#    print(full_processed[col].unique())\n",
    "\n",
    "# Now we know all possible categories that could appear in any fold\n",
    "# We can modify the DataPipeline class to handle these known categories"
   ],
   "id": "6211047d8d00b9f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T19:47:21.189819Z",
     "start_time": "2024-12-19T19:47:21.177665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from colorama import Fore\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def custom_score(solution, submission, row_id_column_name, prediction_label='prediction', print_info=True):\n",
    "\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "\n",
    "    event_label = 'efs'\n",
    "    interval_label = 'efs_time'\n",
    "\n",
    "    for col in submission.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "    # Merging solution and submission dfs on ID\n",
    "    merged_df = pd.concat([solution, submission], axis=1)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_dict = {}\n",
    "    for race in sorted(merged_df_race_dict.keys()):\n",
    "        # Retrieving values from y_test based on index\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        # Calculate the concordance index\n",
    "        c_index_race = concordance_index(\n",
    "            merged_df_race[interval_label],\n",
    "            -merged_df_race[prediction_label],\n",
    "            merged_df_race[event_label])\n",
    "\n",
    "        metric_dict[race] = c_index_race\n",
    "\n",
    "    race_c_index = list(metric_dict.values())\n",
    "    c_score = float(np.mean(race_c_index) - np.std(race_c_index))\n",
    "    if print_info:\n",
    "        print(f\"{Fore.GREEN}{Style.BRIGHT}# c-index={c_score:.4f}, mean={np.mean(race_c_index):.4f} std={np.std(race_c_index):.4f}{Style.RESET_ALL}\")\n",
    "\n",
    "    return c_score, metric_dict\n",
    "\n",
    "\n",
    "def display_overall(df):\n",
    "\n",
    "    race_groups = [\n",
    "        'American Indian or Alaska Native', 'Asian',\n",
    "        'Black or African-American', 'More than one race',\n",
    "        'Native Hawaiian or other Pacific Islander', 'White'\n",
    "    ]\n",
    "    df['mean'] = df[race_groups].mean(axis=1)\n",
    "    df['std'] = np.std(df[race_groups], axis=1)\n",
    "    df['score'] = df['mean'] - df['std']\n",
    "    df = df.T\n",
    "    df['Overall'] = df.mean(axis=1)\n",
    "    temp = df.drop(index=['std']).values\n",
    "    display(df\n",
    "            .iloc[:len(race_groups)]\n",
    "            .style\n",
    "            .format(precision=4)\n",
    "            .background_gradient(axis=None, vmin=temp.min(), vmax=temp.max(), cmap=\"cool\")\n",
    "            .concat(df.iloc[len(race_groups):].style.format(precision=3))\n",
    "            )\n"
   ],
   "id": "c0c9940b624b12ad",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:00:28.092665Z",
     "start_time": "2024-12-19T16:00:28.089732Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "aac21251ed3fd9b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T19:47:26.754259Z",
     "start_time": "2024-12-19T19:47:26.750314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def CIndexMetric_XGB(y_true, y_pred):\n",
    "    ds_pred[\"prediction\"] = y_pred\n",
    "    cindex_score, _ = custom_score(ds_true.copy(), ds_pred.copy(), \"ID\", print_info=False)\n",
    "    return -cindex_score"
   ],
   "id": "c05a0697703bf7c3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T15:15:08.742114Z",
     "start_time": "2024-12-19T15:15:08.740208Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3687893d0be37574",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T15:15:09.359744Z",
     "start_time": "2024-12-19T15:15:09.357996Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "60b9ca2c5f9aaca9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T21:21:17.975348Z",
     "start_time": "2024-12-19T21:19:01.598779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1. Start with raw data \n",
    "train = pd.read_csv(DATA_DIR +'/train.csv')\n",
    "\n",
    "\n",
    "\n",
    "# 2. Create folds\n",
    "folds = 5\n",
    "train['kfold'] = -1\n",
    "\n",
    "#skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "#groups = train['efs'].astype(str)\n",
    "#for fold, (train_idx, val_idx) in enumerate(skf.split(X=train, y=groups)):\n",
    "#    train.loc[val_idx, 'kfold'] = fold\n",
    "\n",
    "groups = train['race_group']\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X=train, y=groups)):\n",
    "    train.loc[val_idx, 'kfold'] = fold\n",
    "\n",
    "# 3. Initialize outputs\n",
    "oof_xgb = train[['kfold','ID','efs','efs_time','race_group']].copy()\n",
    "oof_xgb['prediction'] = 0.0\n",
    "metric_df = []\n",
    "\n",
    "# 4. Training loop\n",
    "for fold in range(skf.n_splits):\n",
    "    pipeline = DataPipeline()\n",
    "\n",
    "    x_train = train[train.kfold != fold].copy()\n",
    "    x_valid = train[train.kfold == fold].copy()\n",
    "\n",
    "    # Process each fold's data first\n",
    "    x_train = pipeline.process_data(x_train, training=True)\n",
    "    x_valid = pipeline.process_data(x_valid, training=False)\n",
    "\n",
    "    # Check all object/categorical columns\n",
    "    #categorical_cols = train.select_dtypes(include=['object', 'category']).columns\n",
    "    #print(\"\\nAll categorical columns:\")\n",
    "    #for col in categorical_cols:\n",
    "    #    print(f\"\\n{col}:\")\n",
    "    #    print(train[col].unique())\n",
    "\n",
    "    # Now define FEATURES after new columns are created\n",
    "    RMV = [\"ID\",\"efs\",\"efs_time\",\"label\",'y','kfold']\n",
    "    FEATURES = [c for c in x_train.columns if not c in RMV]\n",
    "\n",
    "    # Print features on first fold\n",
    "    if fold == 0:\n",
    "        feature_importances_xgb = pd.DataFrame()\n",
    "        feature_importances_xgb['feature'] = FEATURES\n",
    "        print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n",
    "\n",
    "    # Get labels\n",
    "    y_train = x_train['label']\n",
    "    y_valid = x_valid['label']\n",
    "    y_label = x_valid['efs']\n",
    "\n",
    "    # Select features\n",
    "    x_train = x_train[FEATURES]\n",
    "    x_valid = x_valid[FEATURES]\n",
    "\n",
    "    # Handle data types\n",
    "    for c in x_train.columns:\n",
    "        if x_train[c].dtype == \"object\":\n",
    "            x_train[c] = x_train[c].fillna(\"NAN\").astype(\"category\")\n",
    "            x_valid[c] = x_valid[c].fillna(\"NAN\").astype(\"category\")\n",
    "        else:\n",
    "            if x_train[c].dtype==\"float64\":\n",
    "                x_train[c] = x_train[c].replace([np.inf, -np.inf], np.nan).astype(\"float32\")\n",
    "                x_valid[c] = x_valid[c].replace([np.inf, -np.inf], np.nan).astype(\"float32\")\n",
    "            if x_train[c].dtype==\"int64\":\n",
    "                x_train[c] = x_train[c].astype(\"int32\")\n",
    "                x_valid[c] = x_valid[c].astype(\"int32\")\n",
    "\n",
    "\n",
    "    # Set up for metrics\n",
    "    ds_true = oof_xgb.loc[oof_xgb.kfold==fold, [\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy().reset_index(drop=True)\n",
    "    ds_pred = oof_xgb.loc[oof_xgb.kfold==fold, [\"ID\"]].copy().reset_index(drop=True)\n",
    "\n",
    "    # Train model\n",
    "    clf = XGBRegressor(\n",
    "        max_depth=3,\n",
    "        colsample_bytree=0.5,\n",
    "        subsample=0.8,\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.03,\n",
    "        early_stopping_rounds=100,\n",
    "        objective='reg:squarederror',\n",
    "        enable_categorical=True,\n",
    "        min_child_weight=5,\n",
    "        eval_metric=CIndexMetric_XGB,\n",
    "        disable_default_eval_metric=True,\n",
    "        missing=np.nan\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=500,\n",
    "    )\n",
    "\n",
    "    # Get feature importance and predictions\n",
    "    feature_importances_xgb[f'fold_{fold + 1}'] = feature_importances_xgb['feature'].map(clf.get_booster().get_score())\n",
    "    preds_valid = clf.predict(x_valid)\n",
    "    oof_xgb.loc[oof_xgb.kfold==fold, 'prediction'] = preds_valid\n",
    "\n",
    "    # Save model\n",
    "    clf.save_model(f\"xgb_model_{fold}.bin\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    y_true = oof_xgb.loc[oof_xgb.kfold==fold, [\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy().reset_index(drop=True)\n",
    "    y_pred = oof_xgb.loc[oof_xgb.kfold==fold, [\"ID\",\"prediction\"]].copy().reset_index(drop=True)\n",
    "    m, metric_dict = custom_score(y_true, y_pred, \"ID\", print_info=True)\n",
    "    metric_df.append(metric_dict)"
   ],
   "id": "8c8513f20f410243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 92 FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'hla_match_c_high', 'hla_high_res_8', 'tbi_status', 'arrhythmia', 'hla_low_res_6', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'hla_high_res_6', 'cmv_status', 'hla_high_res_10', 'hla_match_dqb1_high', 'tce_imm_match', 'hla_nmdp_6', 'hla_match_c_low', 'rituximab', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'year_hct', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hla_match_a_high', 'hepatic_severe', 'donor_age', 'prior_tumor', 'hla_match_b_low', 'peptic_ulcer', 'age_at_hct', 'hla_match_a_low', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hla_match_b_high', 'race_group', 'comorbidity_score', 'karnofsky_score', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'hla_low_res_8', 'cardiac', 'hla_match_drb1_high', 'pulm_moderate', 'hla_low_res_10', 'hla_match_c_high_missing', 'hla_high_res_8_missing', 'hla_low_res_6_missing', 'hla_high_res_6_missing', 'hla_high_res_10_missing', 'hla_match_dqb1_high_missing', 'hla_nmdp_6_missing', 'hla_match_c_low_missing', 'hla_match_drb1_low_missing', 'hla_match_dqb1_low_missing', 'hla_match_a_high_missing', 'hla_match_b_low_missing', 'hla_match_a_low_missing', 'hla_match_b_high_missing', 'hla_low_res_8_missing', 'hla_match_drb1_high_missing', 'hla_low_res_10_missing', 'hla_testing_completeness', 'conditioning_karnofsky', 'dri_comorbidity', 'age_conditioning', 'hla_high_res_composite', 'hla_low_res_composite', 'patient_risk_score', 'risk_treatment_match', 'transplant_era', 'era_conditioning', 'era_hla_quality', 'gvhd_main_strategy', 'disease_conditioning', 'disease_era', 'hla_mismatch_pattern', 'hla_era_trend', 'comorbidity_pattern', 'age_comorbidity_risk']\n",
      "[0]\tvalidation_0-CIndexMetric_XGB:-0.51646\n",
      "[356]\tvalidation_0-CIndexMetric_XGB:-0.57881\n",
      "\u001B[32m\u001B[1m# c-index=0.5805, mean=0.5946 std=0.0141\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kaggle1/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [22:19:33] WARNING: /var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_b6qk1lz_ug/croot/xgboost-split_1724073748391/work/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-CIndexMetric_XGB:-0.53488\n",
      "[367]\tvalidation_0-CIndexMetric_XGB:-0.59674\n",
      "\u001B[32m\u001B[1m# c-index=0.6000, mean=0.6074 std=0.0074\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kaggle1/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [22:19:59] WARNING: /var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_b6qk1lz_ug/croot/xgboost-split_1724073748391/work/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-CIndexMetric_XGB:-0.54519\n",
      "[348]\tvalidation_0-CIndexMetric_XGB:-0.62261\n",
      "\u001B[32m\u001B[1m# c-index=0.6249, mean=0.6348 std=0.0100\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kaggle1/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [22:20:24] WARNING: /var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_b6qk1lz_ug/croot/xgboost-split_1724073748391/work/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-CIndexMetric_XGB:-0.54298\n",
      "[379]\tvalidation_0-CIndexMetric_XGB:-0.61899\n",
      "\u001B[32m\u001B[1m# c-index=0.6202, mean=0.6273 std=0.0071\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kaggle1/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [22:20:50] WARNING: /var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_b6qk1lz_ug/croot/xgboost-split_1724073748391/work/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-CIndexMetric_XGB:-0.58618\n",
      "[410]\tvalidation_0-CIndexMetric_XGB:-0.65051\n",
      "\u001B[32m\u001B[1m# c-index=0.6515, mean=0.6579 std=0.0064\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kaggle1/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [22:21:17] WARNING: /var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_b6qk1lz_ug/croot/xgboost-split_1724073748391/work/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T21:21:26.175109Z",
     "start_time": "2024-12-19T21:21:26.157686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_importances_xgb['importance'] = feature_importances_xgb.drop('feature', axis=1).mean(axis=1)\n",
    "feature_importances_xgb = feature_importances_xgb.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "feature_importances_xgb.head(20)"
   ],
   "id": "9c0aff9d72b3ddc7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   feature  fold_1  fold_2  fold_3  fold_4  fold_5  importance\n",
       "0              disease_era   397.0   387.0   360.0   405.0   456.0       401.0\n",
       "1     disease_conditioning   354.0   333.0   313.0   368.0   380.0       349.6\n",
       "2          dri_comorbidity   247.0   257.0   276.0   262.0   290.0       266.4\n",
       "3   conditioning_karnofsky   131.0   149.0   127.0   152.0   159.0       143.6\n",
       "4         era_conditioning   116.0   116.0   116.0   113.0   114.0       115.0\n",
       "5                sex_match    96.0   106.0    99.0   105.0   106.0       102.4\n",
       "6         age_conditioning    85.0    98.0    92.0    87.0   106.0        93.6\n",
       "7                 year_hct    92.0    86.0    90.0    94.0    87.0        89.8\n",
       "8      comorbidity_pattern    83.0    90.0    83.0    90.0    74.0        84.0\n",
       "9     risk_treatment_match    66.0    67.0    52.0    59.0    67.0        62.2\n",
       "10               donor_age    57.0    60.0    46.0    76.0    64.0        60.6\n",
       "11              cyto_score    53.0    56.0    56.0    52.0    68.0        57.0\n",
       "12              cmv_status    47.0    60.0    58.0    55.0    55.0        55.0\n",
       "13       cyto_score_detail    58.0    50.0    43.0    56.0    56.0        52.6\n",
       "14              race_group    52.0    50.0    53.0    45.0    59.0        51.8\n",
       "15              gvhd_proph    45.0    49.0    56.0    47.0    56.0        50.6\n",
       "16              tbi_status    45.0    52.0    43.0    45.0    56.0        48.2\n",
       "17         era_hla_quality    37.0    48.0    36.0    58.0    46.0        45.0\n",
       "18      patient_risk_score    44.0    39.0    37.0    50.0    47.0        43.4\n",
       "19         karnofsky_score    36.0    34.0    40.0    51.0    47.0        41.6"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>fold_1</th>\n",
       "      <th>fold_2</th>\n",
       "      <th>fold_3</th>\n",
       "      <th>fold_4</th>\n",
       "      <th>fold_5</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disease_era</td>\n",
       "      <td>397.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>401.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disease_conditioning</td>\n",
       "      <td>354.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>349.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dri_comorbidity</td>\n",
       "      <td>247.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>266.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conditioning_karnofsky</td>\n",
       "      <td>131.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>143.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>era_conditioning</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sex_match</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>102.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>age_conditioning</td>\n",
       "      <td>85.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>93.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>year_hct</td>\n",
       "      <td>92.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>89.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>comorbidity_pattern</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>risk_treatment_match</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>62.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>donor_age</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>60.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cyto_score</td>\n",
       "      <td>53.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cmv_status</td>\n",
       "      <td>47.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cyto_score_detail</td>\n",
       "      <td>58.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>52.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>race_group</td>\n",
       "      <td>52.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>51.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gvhd_proph</td>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>50.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tbi_status</td>\n",
       "      <td>45.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>era_hla_quality</td>\n",
       "      <td>37.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>patient_risk_score</td>\n",
       "      <td>44.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>karnofsky_score</td>\n",
       "      <td>36.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>41.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:53:50.891828Z",
     "start_time": "2024-12-19T16:53:50.728093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_data = pd.read_csv(DATA_DIR +'/test.csv')\n",
    "\n",
    "# Apply the same pipeline\n",
    "pipeline = DataPipeline()\n",
    "new_data = pipeline.process_data(new_data, training=True)\n",
    "\n",
    "# Select FEATURES\n",
    "RMV = [\"ID\", \"efs\", \"efs_time\", \"label\", \"y\", \"kfold\"]\n",
    "FEATURES = [c for c in new_data.columns if c not in RMV]\n",
    "new_data = new_data[FEATURES]\n",
    "\n",
    "# Handle data types (similar to training loop)\n",
    "for c in new_data.columns:\n",
    "    if new_data[c].dtype == \"object\":\n",
    "        new_data[c] = new_data[c].fillna(\"NAN\").astype(\"category\")\n",
    "    else:\n",
    "        if new_data[c].dtype == \"float64\":\n",
    "            new_data[c] = new_data[c].replace([np.inf, -np.inf], np.nan).astype(\"float32\")\n",
    "        if new_data[c].dtype == \"int64\":\n",
    "            new_data[c] = new_data[c].astype(\"int32\")\n",
    "\n",
    "clf.predict(new_data)"
   ],
   "id": "9bae8d4e1f48c498",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48765048, 0.61907995, 0.5356562 ], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T19:12:46.646292Z",
     "start_time": "2024-12-19T19:12:46.615254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "import optuna\n",
    "from typing import Dict, List, Tuple\n",
    "import logging\n",
    "\n",
    "class XGBoostTuner:\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_data: pd.DataFrame,\n",
    "            pipeline: DataPipeline,\n",
    "            fold_column: str = 'kfold',\n",
    "            n_trials: int = 100,\n",
    "            random_state: int = 42\n",
    "    ):\n",
    "        self.train_data = train_data\n",
    "        self.pipeline = pipeline\n",
    "        self.fold_column = fold_column\n",
    "        self.n_trials = n_trials\n",
    "        self.random_state = random_state\n",
    "        self.best_params = None\n",
    "        self.study = None\n",
    "\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def _handle_nan_values(self, true_data: pd.DataFrame, pred_data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Handle NaN values in the data before calculating metrics.\n",
    "        \"\"\"\n",
    "        # Get valid indices (non-NaN in both true and predicted)\n",
    "        valid_mask = (\n",
    "                ~true_data['efs'].isna() &\n",
    "                ~true_data['efs_time'].isna() &\n",
    "                ~pred_data['prediction'].isna()\n",
    "        )\n",
    "\n",
    "        # Filter both dataframes\n",
    "        true_data_clean = true_data[valid_mask].copy()\n",
    "        pred_data_clean = pred_data[valid_mask].copy()\n",
    "\n",
    "        return true_data_clean, pred_data_clean\n",
    "\n",
    "    def _objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"\n",
    "        Objective function for Optuna optimization.\n",
    "        \"\"\"\n",
    "        # Define parameter search space\n",
    "        param = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 1000, 20000),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.8),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "            'early_stopping_rounds': 100,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'enable_categorical': True,\n",
    "            'eval_metric': CIndexMetric_XGB,\n",
    "            'disable_default_eval_metric': True,\n",
    "            'missing': np.nan\n",
    "        }\n",
    "\n",
    "        # Cross-validation scores for each fold\n",
    "        cv_scores = []\n",
    "\n",
    "        # Perform cross-validation\n",
    "        for fold in range(self.train_data[self.fold_column].nunique()):\n",
    "            # Split data\n",
    "            x_train = self.train_data[self.train_data[self.fold_column] != fold].copy()\n",
    "            x_valid = self.train_data[self.train_data[self.fold_column] == fold].copy()\n",
    "\n",
    "            # Store necessary columns for metrics before processing\n",
    "            valid_metrics = x_valid[['ID', 'efs', 'efs_time', 'race_group']].copy()\n",
    "\n",
    "            # Process each fold's data using the pipeline\n",
    "            x_train = self.pipeline.process_data(x_train, training=True)\n",
    "            x_valid = self.pipeline.process_data(x_valid, training=False)\n",
    "\n",
    "            # Get labels\n",
    "            y_train = x_train['label']\n",
    "            y_valid = x_valid['label']\n",
    "\n",
    "            # Define features after pipeline processing\n",
    "            RMV = [\"ID\", \"efs\", \"efs_time\", \"label\", 'y', 'kfold']\n",
    "            FEATURES = [c for c in x_train.columns if c not in RMV]\n",
    "\n",
    "            # Select features\n",
    "            x_train = x_train[FEATURES]\n",
    "            x_valid = x_valid[FEATURES]\n",
    "\n",
    "            # Handle data types\n",
    "            for c in x_train.columns:\n",
    "                if x_train[c].dtype == \"object\":\n",
    "                    x_train[c] = x_train[c].fillna(\"NAN\").astype(\"category\")\n",
    "                    x_valid[c] = x_valid[c].fillna(\"NAN\").astype(\"category\")\n",
    "                else:\n",
    "                    if x_train[c].dtype == \"float64\":\n",
    "                        x_train[c] = x_train[c].replace([np.inf, -np.inf], np.nan).astype(\"float32\")\n",
    "                        x_valid[c] = x_valid[c].replace([np.inf, -np.inf], np.nan).astype(\"float32\")\n",
    "                    if x_train[c].dtype == \"int64\":\n",
    "                        x_train[c] = x_train[c].astype(\"int32\")\n",
    "                        x_valid[c] = x_valid[c].astype(\"int32\")\n",
    "\n",
    "            # Train model\n",
    "            model = XGBRegressor(**param)\n",
    "            model.fit(\n",
    "                x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Get predictions\n",
    "            preds = model.predict(x_valid)\n",
    "\n",
    "            # Calculate custom metric using stored metrics data\n",
    "            y_pred = pd.DataFrame({\n",
    "                \"ID\": valid_metrics[\"ID\"].values,\n",
    "                \"prediction\": preds\n",
    "            })\n",
    "\n",
    "            # Handle NaN values before calculating metrics\n",
    "            true_clean, pred_clean = self._handle_nan_values(valid_metrics, y_pred)\n",
    "\n",
    "            if len(true_clean) > 0:  # Only calculate metric if we have valid data\n",
    "                metric, _ = custom_score(true_clean, pred_clean, \"ID\", print_info=False)\n",
    "                cv_scores.append(metric)\n",
    "            else:\n",
    "                self.logger.warning(f\"No valid data for fold {fold} after handling NaN values\")\n",
    "                cv_scores.append(0.0)  # or some other appropriate default value\n",
    "\n",
    "        mean_score = np.mean(cv_scores)\n",
    "        self.logger.info(f\"Trial finished with score: {mean_score:.4f}\")\n",
    "        return mean_score\n",
    "\n",
    "    def optimize(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Run the optimization process.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting hyperparameter optimization...\")\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=\"xgboost_optimization\"\n",
    "        )\n",
    "\n",
    "        study.optimize(\n",
    "            self._objective,\n",
    "            n_trials=self.n_trials,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        self.best_params = study.best_params\n",
    "        self.study = study\n",
    "\n",
    "        self.logger.info(f\"Best score: {study.best_value:.4f}\")\n",
    "        self.logger.info(\"Best hyperparameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            self.logger.info(f\"    {key}: {value}\")\n",
    "\n",
    "        return self.best_params\n",
    "\n",
    "    def plot_optimization_history(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot the optimization history using Optuna's visualization tools.\n",
    "        \"\"\"\n",
    "        if self.study is None:\n",
    "            raise ValueError(\"No study available. Run optimize() first.\")\n",
    "\n",
    "        try:\n",
    "            from optuna.visualization import plot_optimization_history\n",
    "            fig = plot_optimization_history(self.study)\n",
    "            fig.show()\n",
    "        except ImportError:\n",
    "            self.logger.warning(\"Plotly is required for visualization. Install with: pip install plotly\")"
   ],
   "id": "1fd0806a33951b67",
   "outputs": [],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T19:33:03.285605Z",
     "start_time": "2024-12-19T19:33:03.259591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "import optuna\n",
    "from typing import Dict, List, Tuple\n",
    "import logging\n",
    "\n",
    "class XGBoostTuner:\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_data: pd.DataFrame,\n",
    "            pipeline: DataPipeline,\n",
    "            fold_column: str = 'kfold',\n",
    "            n_trials: int = 100,\n",
    "            random_state: int = 42\n",
    "    ):\n",
    "        self.train_data = train_data\n",
    "        self.pipeline = pipeline\n",
    "        self.fold_column = fold_column\n",
    "        self.n_trials = n_trials\n",
    "        self.random_state = random_state\n",
    "        self.best_params = None\n",
    "        self.study = None\n",
    "\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def _handle_nan_values(self, true_data: pd.DataFrame, pred_data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Handle NaN values in the data before calculating metrics.\"\"\"\n",
    "        valid_mask = (\n",
    "                ~true_data['efs'].isna() &\n",
    "                ~true_data['efs_time'].isna() &\n",
    "                ~pred_data['prediction'].isna()\n",
    "        )\n",
    "\n",
    "        true_data_clean = true_data[valid_mask].copy()\n",
    "        pred_data_clean = pred_data[valid_mask].copy()\n",
    "\n",
    "        return true_data_clean, pred_data_clean\n",
    "\n",
    "    def _objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"\n",
    "        Objective function for Optuna optimization.\n",
    "        Returns the negative of (mean - std) of race-stratified c-indices\n",
    "        to match the original metric convention.\n",
    "        \"\"\"\n",
    "        param = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 4),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 8000, 12000),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 3, 7),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 0.6),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n",
    "            'early_stopping_rounds': 100,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'enable_categorical': True,\n",
    "            'eval_metric': CIndexMetric_XGB,\n",
    "            'disable_default_eval_metric': True,\n",
    "            'missing': np.nan\n",
    "        }\n",
    "\n",
    "        cv_scores = []\n",
    "\n",
    "        for fold in range(self.train_data[self.fold_column].nunique()):\n",
    "            x_train = self.train_data[self.train_data[self.fold_column] != fold].copy()\n",
    "            x_valid = self.train_data[self.train_data[self.fold_column] == fold].copy()\n",
    "\n",
    "            valid_metrics = x_valid[['ID', 'efs', 'efs_time', 'race_group']].copy()\n",
    "\n",
    "            x_train = self.pipeline.process_data(x_train, training=True)\n",
    "            x_valid = self.pipeline.process_data(x_valid, training=False)\n",
    "\n",
    "            y_train = x_train['label']\n",
    "            y_valid = x_valid['label']\n",
    "\n",
    "            RMV = [\"ID\", \"efs\", \"efs_time\", \"label\", 'y', 'kfold']\n",
    "            FEATURES = [c for c in x_train.columns if c not in RMV]\n",
    "\n",
    "            x_train = x_train[FEATURES]\n",
    "            x_valid = x_valid[FEATURES]\n",
    "\n",
    "            for c in x_train.columns:\n",
    "                if x_train[c].dtype == \"object\":\n",
    "                    x_train[c] = x_train[c].fillna(\"NAN\").astype(\"category\")\n",
    "                    x_valid[c] = x_valid[c].fillna(\"NAN\").astype(\"category\")\n",
    "                else:\n",
    "                    if x_train[c].dtype == \"float64\":\n",
    "                        x_train[c] = x_train[c].replace([np.inf, -np.inf], np.nan).astype(\"float32\")\n",
    "                        x_valid[c] = x_valid[c].replace([np.inf, -np.inf], np.nan).astype(\"float32\")\n",
    "                    if x_train[c].dtype == \"int64\":\n",
    "                        x_train[c] = x_train[c].astype(\"int32\")\n",
    "                        x_valid[c] = x_valid[c].astype(\"int32\")\n",
    "\n",
    "            model = XGBRegressor(**param)\n",
    "            model.fit(\n",
    "                x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            preds = model.predict(x_valid)\n",
    "\n",
    "            y_pred = pd.DataFrame({\n",
    "                \"ID\": valid_metrics[\"ID\"].values,\n",
    "                \"prediction\": preds\n",
    "            })\n",
    "\n",
    "            true_clean, pred_clean = self._handle_nan_values(valid_metrics, y_pred)\n",
    "\n",
    "            if len(true_clean) > 0:\n",
    "                metric, _ = custom_score(true_clean, pred_clean, \"ID\", print_info=False)\n",
    "                cv_scores.append(metric)\n",
    "            else:\n",
    "                self.logger.warning(f\"No valid data for fold {fold} after handling NaN values\")\n",
    "                cv_scores.append(0.0)\n",
    "\n",
    "        mean_score = np.mean(cv_scores)\n",
    "        self.logger.info(f\"Trial finished with score: {mean_score:.4f}\")\n",
    "        return mean_score\n",
    "\n",
    "    def optimize(self) -> Dict:\n",
    "        \"\"\"Run the optimization process.\"\"\"\n",
    "        self.logger.info(\"Starting hyperparameter optimization...\")\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",  # Because both CIndexMetric_XGB and custom_score return negative values\n",
    "            study_name=\"xgboost_optimization\"\n",
    "        )\n",
    "\n",
    "        study.optimize(\n",
    "            self._objective,\n",
    "            n_trials=self.n_trials,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        self.best_params = {\n",
    "            'max_depth': study.best_params['max_depth'],\n",
    "            'learning_rate': study.best_params['learning_rate'],\n",
    "            'n_estimators': study.best_params['n_estimators'],\n",
    "            'min_child_weight': study.best_params['min_child_weight'],\n",
    "            'colsample_bytree': study.best_params['colsample_bytree'],\n",
    "            'subsample': study.best_params['subsample'],\n",
    "            'early_stopping_rounds': 100,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'enable_categorical': True,\n",
    "            'eval_metric': CIndexMetric_XGB,\n",
    "            'disable_default_eval_metric': True,\n",
    "            'missing': np.nan\n",
    "        }\n",
    "\n",
    "        self.study = study\n",
    "\n",
    "        self.logger.info(f\"Best score: {study.best_value:.4f}\")\n",
    "        self.logger.info(\"Best hyperparameters:\")\n",
    "        for key, value in self.best_params.items():\n",
    "            self.logger.info(f\"    {key}: {value}\")\n",
    "\n",
    "        return self.best_params\n",
    "\n",
    "    def plot_optimization_history(self) -> None:\n",
    "        \"\"\"Plot the optimization history.\"\"\"\n",
    "        if self.study is None:\n",
    "            raise ValueError(\"No study available. Run optimize() first.\")\n",
    "\n",
    "        try:\n",
    "            from optuna.visualization import plot_optimization_history\n",
    "            fig = plot_optimization_history(self.study)\n",
    "            fig.show()\n",
    "        except ImportError:\n",
    "            self.logger.warning(\"Plotly is required for visualization. Install with: pip install plotly\")\n",
    "\n",
    "    def get_best_params(self) -> Dict:\n",
    "        \"\"\"Return the best parameters found during optimization.\"\"\"\n",
    "        if self.best_params is None:\n",
    "            raise ValueError(\"No best parameters available. Run optimize() first.\")\n",
    "        return self.best_params"
   ],
   "id": "2fdfd2ea270c93b7",
   "outputs": [],
   "execution_count": 215
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "98af293e9116a236"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-19T19:33:14.637456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize pipeline and tuner\n",
    "pipeline = DataPipeline()\n",
    "tuner = XGBoostTuner(\n",
    "    train_data=train,\n",
    "    pipeline=pipeline,\n",
    "    n_trials=100\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "tuner.optimize()\n",
    "\n",
    "# Get best parameters\n",
    "best_params = tuner.get_best_params()\n",
    "\n",
    "# Create validation dataframe for metric calculation\n",
    "ds_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "ds_pred = train[[\"ID\"]].copy()\n",
    "\n",
    "# Initialize model with best parameters\n",
    "clf = XGBRegressor(**best_params)\n",
    "\n",
    "# Train the model (your original training loop)\n",
    "clf.fit(\n",
    "    x_train, y_train,\n",
    "    eval_set=[(x_valid, y_valid)],\n",
    "    verbose=500\n",
    ")"
   ],
   "id": "9296b552825adf48",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting hyperparameter optimization...\n",
      "[I 2024-12-19 20:33:14,656] A new study created in memory with name: xgboost_optimization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2f0b75f400344e2bf410336cfec54c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lp/38fnv8v91rqdkwbf17xlnxyh0000gn/T/ipykernel_5143/2265414477.py:39: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  true_data_clean = true_data[valid_mask].copy()\n",
      "/var/folders/lp/38fnv8v91rqdkwbf17xlnxyh0000gn/T/ipykernel_5143/2265414477.py:40: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  pred_data_clean = pred_data[valid_mask].copy()\n",
      "/var/folders/lp/38fnv8v91rqdkwbf17xlnxyh0000gn/T/ipykernel_5143/2265414477.py:39: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  true_data_clean = true_data[valid_mask].copy()\n",
      "/var/folders/lp/38fnv8v91rqdkwbf17xlnxyh0000gn/T/ipykernel_5143/2265414477.py:40: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  pred_data_clean = pred_data[valid_mask].copy()\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T17:17:15.542953Z",
     "start_time": "2024-12-19T17:17:15.528880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def generate_predictions(\n",
    "        test_path: str,\n",
    "        model_paths: list,\n",
    "        pipeline: DataPipeline,\n",
    "        submission_path: str = \"submission.csv\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate predictions for test data using an ensemble of trained models.\n",
    "    \n",
    "    Args:\n",
    "        test_path: Path to test CSV file\n",
    "        model_paths: List of paths to saved XGBoost models (one per fold)\n",
    "        pipeline: Trained DataPipeline instance\n",
    "        submission_path: Path to save submission file\n",
    "    \"\"\"\n",
    "    # Read test data\n",
    "    test = pd.read_csv(test_path)\n",
    "    test_processed = pipeline.process_data(test, training=False)\n",
    "\n",
    "    # Define features (same as training)\n",
    "    RMV = [\"ID\", \"efs\", \"efs_time\", \"label\", 'y', 'kfold']\n",
    "    FEATURES = [c for c in test_processed.columns if not c in RMV]\n",
    "\n",
    "    # Handle data types\n",
    "    for c in FEATURES:\n",
    "        if test_processed[c].dtype == \"object\":\n",
    "            test_processed[c] = test_processed[c].fillna(\"NAN\").astype(\"category\")\n",
    "        else:\n",
    "            if test_processed[c].dtype == \"float64\":\n",
    "                test_processed[c] = test_processed[c].replace([np.inf, -np.inf], np.nan).astype(\"float32\")\n",
    "            if test_processed[c].dtype == \"int64\":\n",
    "                test_processed[c] = test_processed[c].astype(\"int32\")\n",
    "\n",
    "    # Get features for prediction\n",
    "    X_test = test_processed[FEATURES]\n",
    "\n",
    "    # Make predictions with each model\n",
    "    predictions = []\n",
    "    for model_path in model_paths:\n",
    "        model = XGBRegressor()\n",
    "        model.load_model(model_path)\n",
    "        pred = model.predict(X_test)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Average predictions from all models\n",
    "    final_predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test['ID'],\n",
    "        'prediction': final_predictions\n",
    "    })\n",
    "\n",
    "    # Save submission\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "    # Print some basic statistics\n",
    "    print(\"\\nPrediction Statistics:\")\n",
    "    print(submission['prediction'].describe())\n",
    "\n",
    "    return submission\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# After training your models\n",
    "pipeline = DataPipeline()  # Your trained pipeline\n",
    "\n",
    "# List of your saved model paths\n",
    "model_paths = [f\"xgb_model_{fold}.bin\" for fold in range(5)]\n",
    "\n",
    "# Generate predictions\n",
    "submission = generate_predictions(\n",
    "    test_path='test.csv',\n",
    "    model_paths=model_paths,\n",
    "    pipeline=pipeline,\n",
    "    submission_path='submission.csv'\n",
    ")\n",
    "\"\"\""
   ],
   "id": "b82c1907507c409c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "execution_count": 197
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
